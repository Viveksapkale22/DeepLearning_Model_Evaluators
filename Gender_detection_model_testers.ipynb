{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1cgQ-JTUuwYaiKkMO1PcsEqEjHvwITc8T",
      "authorship_tag": "ABX9TyNnhfwfczmpBU9oAezzU2lB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viveksapkale22/DeepLearning_Model_Evaluators/blob/main/Gender_detection_model_testers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test-1 : haarcascade_frontalface_default + own gender detection model V13-mobilenetV3  \n",
        "\n",
        "result :\n",
        "- capture the face perfectly accuracy result 75-90\n",
        "\n",
        "issue :\n",
        "- can't track the face perfectly\n",
        "- [chcek multiple face detection]\n",
        "- [chcek try from distance]\n",
        "- [chcek any miss-match issue chcek]"
      ],
      "metadata": {
        "id": "ZyLyyb9UoXkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow opencv-python-headless pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "174r97mzcuVS",
        "outputId": "b2067ae0-1e12-4a56-90dc-7f42649f050a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.13.0.90)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL.Image\n",
        "import io\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "1qobjgXtdXLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert JavaScript webcam image to OpenCV BGR image\n",
        "def js_to_image(js_reply):\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "    return img\n",
        "\n",
        "# Convert overlay bounding boxes to base64 bytes for Colab\n",
        "def bbox_to_bytes(bbox_array):\n",
        "    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    bbox_PIL.save(iobuf, format='PNG')\n",
        "    bbox_bytes = 'data:image/png;base64,{}'.format(b64encode(iobuf.getvalue()).decode('utf-8'))\n",
        "    return bbox_bytes"
      ],
      "metadata": {
        "id": "-8M2uM_UdYkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ],
      "metadata": {
        "id": "I7wCYiCDdaRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def video_stream_colab():\n",
        "    js = Javascript('''\n",
        "    var video; var div = null; var stream; var captureCanvas; var imgElement; var labelElement;\n",
        "    var pendingResolve = null; var shutdown = false;\n",
        "    function removeDom() {stream.getVideoTracks()[0].stop(); video.remove(); div.remove(); video = null; div = null; stream = null; imgElement = null; captureCanvas = null; labelElement = null;}\n",
        "    function onAnimationFrame() { if(!shutdown){window.requestAnimationFrame(onAnimationFrame);} if(pendingResolve){var result=\"\"; if(!shutdown){captureCanvas.getContext('2d').drawImage(video,0,0,640,480); result = captureCanvas.toDataURL('image/jpeg',0.8);} var lp=pendingResolve; pendingResolve=null; lp(result);} }\n",
        "    async function createDom(){ if(div!==null){return stream;} div=document.createElement('div'); div.style.border='2px solid black'; div.style.padding='3px'; div.style.width='100%'; div.style.maxWidth='600px'; document.body.appendChild(div); const modelOut=document.createElement('div'); modelOut.innerHTML=\"<span>Status:</span>\"; labelElement=document.createElement('span'); labelElement.innerText='No data'; labelElement.style.fontWeight='bold'; modelOut.appendChild(labelElement); div.appendChild(modelOut); video=document.createElement('video'); video.style.display='block'; video.width=div.clientWidth-6; video.setAttribute('playsinline',''); video.onclick=()=>{shutdown=true;}; stream=await navigator.mediaDevices.getUserMedia({video:{facingMode:\"user\"}}); div.appendChild(video); imgElement=document.createElement('img'); imgElement.style.position='absolute'; imgElement.style.zIndex=1; imgElement.onclick=()=>{shutdown=true;}; div.appendChild(imgElement); const instruction=document.createElement('div'); instruction.innerHTML='<span style=\"color:red;font-weight:bold;\">Click video to stop demo</span>'; div.appendChild(instruction); instruction.onclick=()=>{shutdown=true;}; video.srcObject=stream; await video.play(); captureCanvas=document.createElement('canvas'); captureCanvas.width=640; captureCanvas.height=480; window.requestAnimationFrame(onAnimationFrame); return stream;}\n",
        "    async function stream_frame(label,imgData){ if(shutdown){removeDom(); shutdown=false; return ''; } var preCreate=Date.now(); stream=await createDom(); var preShow=Date.now(); if(label!=\"\"){labelElement.innerHTML=label;} if(imgData!=\"\"){var videoRect=video.getClientRects()[0]; imgElement.style.top=videoRect.top+\"px\"; imgElement.style.left=videoRect.left+\"px\"; imgElement.style.width=videoRect.width+\"px\"; imgElement.style.height=videoRect.height+\"px\"; imgElement.src=imgData;} var preCapture=Date.now(); var result=await new Promise(function(resolve,reject){pendingResolve=resolve;}); shutdown=false; return {'create':preShow-preCreate,'show':preCapture-preShow,'capture':Date.now()-preCapture,'img':result};}\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "def video_frame_colab(label, bbox):\n",
        "    data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "    return data"
      ],
      "metadata": {
        "id": "taIrUZ9wdb0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess face image for MobileNetV3 Small\n",
        "def preprocess_image(img, target_size=(224,224)):\n",
        "    img_resized = cv2.resize(img, target_size)\n",
        "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
        "    img_normalized = img_rgb / 255.0\n",
        "    return np.expand_dims(img_normalized, axis=0)\n",
        "\n",
        "# Predict gender using the model\n",
        "def predict_gender(face_img, model):\n",
        "    input_img = preprocess_image(face_img)\n",
        "    pred = model.predict(input_img)[0][0]  # Assuming sigmoid output\n",
        "    gender = 'Male' if pred > 0.5 else 'Female'\n",
        "    confidence = pred if pred > 0.5 else 1 - pred\n",
        "    return gender, confidence"
      ],
      "metadata": {
        "id": "hnj5FZerddeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def colab_webcam_gender_detection(gender_model):\n",
        "    \"\"\"\n",
        "    Run webcam in Google Colab, detect faces, and predict gender in real-time.\n",
        "    Click on the video to stop the stream.\n",
        "    \"\"\"\n",
        "    video_stream_colab()\n",
        "    label_html = 'Detecting Faces...'\n",
        "    bbox = ''\n",
        "\n",
        "    while True:\n",
        "        js_reply = video_frame_colab(label_html, bbox)\n",
        "        if not js_reply:\n",
        "            break\n",
        "\n",
        "        img = js_to_image(js_reply[\"img\"])\n",
        "        bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray)\n",
        "\n",
        "        for (x,y,w,h) in faces:\n",
        "            face_img = img[y:y+h, x:x+w]\n",
        "            gender, conf = predict_gender(face_img, gender_model)\n",
        "            label = f\"{gender} {conf:.2f}\"\n",
        "            bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "            cv2.putText(bbox_array, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1)\n",
        "\n",
        "        bbox_array[:,:,3] = (bbox_array.max(axis=2)>0).astype(int)*255\n",
        "        bbox = bbox_to_bytes(bbox_array)"
      ],
      "metadata": {
        "id": "2f-G-YFUdgr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load your saved MobileNetV3 Small gender model\n",
        "model_path = '/content/drive/MyDrive/Sem 7/Major project/best_gender_model_e12_v13.keras'\n",
        "gender_model = load_model(model_path)\n",
        "\n",
        "# Run webcam with gender detection\n",
        "colab_webcam_gender_detection(gender_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "xZ7jcDhidiK0",
        "outputId": "4ec979f8-b8e8-476b-9244-6f5abf977ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video; var div = null; var stream; var captureCanvas; var imgElement; var labelElement;\n",
              "    var pendingResolve = null; var shutdown = false;\n",
              "    function removeDom() {stream.getVideoTracks()[0].stop(); video.remove(); div.remove(); video = null; div = null; stream = null; imgElement = null; captureCanvas = null; labelElement = null;}\n",
              "    function onAnimationFrame() { if(!shutdown){window.requestAnimationFrame(onAnimationFrame);} if(pendingResolve){var result=\"\"; if(!shutdown){captureCanvas.getContext('2d').drawImage(video,0,0,640,480); result = captureCanvas.toDataURL('image/jpeg',0.8);} var lp=pendingResolve; pendingResolve=null; lp(result);} }\n",
              "    async function createDom(){ if(div!==null){return stream;} div=document.createElement('div'); div.style.border='2px solid black'; div.style.padding='3px'; div.style.width='100%'; div.style.maxWidth='600px'; document.body.appendChild(div); const modelOut=document.createElement('div'); modelOut.innerHTML=\"<span>Status:</span>\"; labelElement=document.createElement('span'); labelElement.innerText='No data'; labelElement.style.fontWeight='bold'; modelOut.appendChild(labelElement); div.appendChild(modelOut); video=document.createElement('video'); video.style.display='block'; video.width=div.clientWidth-6; video.setAttribute('playsinline',''); video.onclick=()=>{shutdown=true;}; stream=await navigator.mediaDevices.getUserMedia({video:{facingMode:\"user\"}}); div.appendChild(video); imgElement=document.createElement('img'); imgElement.style.position='absolute'; imgElement.style.zIndex=1; imgElement.onclick=()=>{shutdown=true;}; div.appendChild(imgElement); const instruction=document.createElement('div'); instruction.innerHTML='<span style=\"color:red;font-weight:bold;\">Click video to stop demo</span>'; div.appendChild(instruction); instruction.onclick=()=>{shutdown=true;}; video.srcObject=stream; await video.play(); captureCanvas=document.createElement('canvas'); captureCanvas.width=640; captureCanvas.height=480; window.requestAnimationFrame(onAnimationFrame); return stream;}\n",
              "    async function stream_frame(label,imgData){ if(shutdown){removeDom(); shutdown=false; return ''; } var preCreate=Date.now(); stream=await createDom(); var preShow=Date.now(); if(label!=\"\"){labelElement.innerHTML=label;} if(imgData!=\"\"){var videoRect=video.getClientRects()[0]; imgElement.style.top=videoRect.top+\"px\"; imgElement.style.left=videoRect.left+\"px\"; imgElement.style.width=videoRect.width+\"px\"; imgElement.style.height=videoRect.height+\"px\"; imgElement.src=imgData;} var preCapture=Date.now(); var result=await new Promise(function(resolve,reject){pendingResolve=resolve;}); shutdown=false; return {'create':preShow-preCreate,'show':preCapture-preShow,'capture':Date.now()-preCapture,'img':result};}\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1371718878.py:10: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hj8WWXhpL_66"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}